{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtQjdPObtp+GZhoaCmvTil",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMSBornot/PytorchCourseDL/blob/main/simple_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "fh3wRXT04gIU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read MNIS images data into Tensors\n",
        "ttfunc = transforms.ToTensor() # transform to tensor function\n",
        "train_data = datasets.MNIST(root='input_data', train=True, download=True, transform=ttfunc)\n",
        "test_data = datasets.MNIST(root='input_data', train=False, download=True, transform=ttfunc)\n",
        "print(train_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si_lOhgB5dPl",
        "outputId": "79bffba3-7a80-4d8e-a50b-75f4554a835d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: input_data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: input_data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CNN model\n",
        "cnv1 = nn.Conv2d(1, 6, 3, 1) # 1 indicates 1 image as input, 6 for channels/filters, 3x3 for kernel size, and stride=1 in this order\n",
        "cnv2 = nn.Conv2d(6, 16, 3, 1) # input are the 6 channels/filters from cnv1, and it uses 16 channels/filters with 3x3 kernel"
      ],
      "metadata": {
        "id": "VGuk7xJOD8X5"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab 1 MNIST record/image\n",
        "for i, (X_train, y_train) in enumerate(train_data):\n",
        "  break\n",
        "X_train.shape\n",
        "x = X_train.view(1,1,28,28) # reshape the tensor\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vd_EmdwUl-w",
        "outputId": "0915d2e2-be6d-41f0-db5f-63c18a530a4d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the first convolution operation\n",
        "x = F.relu(cnv1(x))\n",
        "print(x.shape)\n",
        "# Pass through the pooling layer\n",
        "x = F.max_pool2d(x,2,2)\n",
        "print(x.shape)\n",
        "# Second convolution operation\n",
        "x = F.relu(cnv2(x))\n",
        "print(x.shape)\n",
        "# Pass through the pooling layer\n",
        "x = F.max_pool2d(x,2,2)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vNVR7cdWyEq",
        "outputId": "5ee396a0-1977-4d26-de66-a53934b8b684"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 26, 26])\n",
            "torch.Size([1, 6, 13, 13])\n",
            "torch.Size([1, 16, 11, 11])\n",
            "torch.Size([1, 16, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class CANN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() # always first instantiate the ancestor class\n",
        "    # covolution layers separated by pooling layers\n",
        "    self.cnv1 = nn.Conv2d(1,6,3,1)\t# input 1x1x28x28 -> 1x6x26x26\n",
        "    self.cnv2 = nn.Conv2d(6,16,3,1)\t# input 1x6x13x13 -> 1x16x11x11 (between cnv1 and cnv2 layers, there is a dropout layer reducing the size 50%)\n",
        "    # fully connected layers stacked at the end, receiving flattened convolution outcome\n",
        "    self.fc1 = nn.Linear(16*5*5,120)\t# input 1x16x5x5 -> 1x120 (between cnv2 and fc1 layers, there is a dropout layer reducing the size 50%)\n",
        "    self.fc2 = nn.Linear(120,84)\t# input 1x120 -> 1x84\n",
        "    self.fc3 = nn.Linear(84,10)\t\t# input 1x84 -> 1x10\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Convolution layers processing\n",
        "    x = F.relu(self.cnv1(x))\n",
        "    x = F.max_pool2d(x,2,2)\n",
        "    x = F.relu(self.cnv2(x))\n",
        "    x = F.max_pool2d(x,2,2)\n",
        "\n",
        "    # flatten conv layers output\n",
        "    x = x.view(-1,16*5*5) # treating the first dimension as unknown (-1) so we can vary the batch size\n",
        "\n",
        "    # Fully connected layers processing\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return F.log_softmax(x,dim=1)"
      ],
      "metadata": {
        "id": "gxMUo35ndsbn"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and instance and use the model\n",
        "torch.manual_seed(41)\n",
        "model = CANN()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4sTaSGXdvNq",
        "outputId": "8f34d86f-b780-415b-be3e-5fdda128d601"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CANN(\n",
              "  (cnv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (cnv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "Q41b12ndgLvv"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Control variables\n",
        "Ne = 10                # Number of epochs\n",
        "Nb = 20               # Batch size\n",
        "trainLoss = []\n",
        "testLoss = []\n",
        "trainAcc = []\n",
        "testAcc = []\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=Nb, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=Nb, shuffle=False)\n",
        "\n",
        "Ntr = train_loader.dataset.train_data.size(0)\n",
        "Nte = test_loader.dataset.test_data.size(0)\n",
        "\n",
        "### Training CNN model\n",
        "T0 = time.time()\n",
        "\n",
        "for i in range(Ne): # epochs loop\n",
        "  acc_train = 0\n",
        "  acc_test = 0\n",
        "  num = 0\n",
        "\n",
        "  # Evaluate for each batch's samples\n",
        "  for j, (Xtr, ytr) in enumerate(train_loader):\n",
        "    # Propagate forward and calculate the loss\n",
        "    out = model(Xtr)  # not flattenet: dim is Nb x 10\n",
        "    loss = criterion(out, ytr)\n",
        "\n",
        "    # Accumulate classification accuracy\n",
        "    _, ind = torch.max(out.data, 1, keepdim=True) # ind's dimension is Nb x 1. If only one output argument is specified then the output will be a tuple (max_value, ind)\n",
        "    acc_train += (ytr.reshape(-1,1) == ind.reshape(-1,1)).sum()\n",
        "\n",
        "    # Update CNN model parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Learning feedback\n",
        "    if (j+1) % 500 == 0: # it runs for 3000 iterations: 3000 x (20 batch samples) = 60,000 MNIST images\n",
        "      print(f\"Epoch {i}, Batch {j+1}: Loss = {loss.data}\")\n",
        "\n",
        "  trainLoss.append(loss)\n",
        "  trainAcc.append(acc_train)\n",
        "\n",
        "  # Evaluation on test data\n",
        "  with torch.no_grad(): # switch off gradient calculation for efficiency\n",
        "    for j, (Xte, yte) in enumerate(test_loader):\n",
        "      out = model(Xte)\n",
        "      _, ind = torch.max(out.data, 1, keepdim=True)\n",
        "      acc_test += (yte.reshape(-1,1) == ind.reshape(-1,1)).sum()\n",
        "\n",
        "  loss = criterion(out, yte)\n",
        "  testLoss.append(loss)\n",
        "  testAcc.append(acc_test)\n",
        "\n",
        "  print(f\"Epoch {i}, Train_Loss = {trainLoss[-1]}, Test_Loss = {testLoss[-1]}, Train_Acc = {trainAcc[-1]/Ntr}, Test_Acc = {testAcc[-1]/Nte}\")\n",
        "\n",
        "total =  time.time() - T0\n",
        "print(f\"Training took: {total/60} minutes!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEE5QA7w5oPM",
        "outputId": "859220f0-d283-4e38-f5bd-a605bd61ccb9"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 500: Loss = 0.6200645565986633\n",
            "Epoch 0, Batch 1000: Loss = 0.32105380296707153\n",
            "Epoch 0, Batch 1500: Loss = 0.31112587451934814\n",
            "Epoch 0, Batch 2000: Loss = 0.018337208777666092\n",
            "Epoch 0, Batch 2500: Loss = 0.09715411067008972\n",
            "Epoch 0, Batch 3000: Loss = 0.007037168834358454\n",
            "Epoch 0, Train_Loss = 0.007037168834358454, Test_Loss = 0.2056254893541336, Train_Acc = 0.9263499975204468, Test_Acc = 0.9760000109672546\n",
            "Epoch 1, Batch 500: Loss = 0.018876830115914345\n",
            "Epoch 1, Batch 1000: Loss = 0.4013398289680481\n",
            "Epoch 1, Batch 1500: Loss = 0.0208301804959774\n",
            "Epoch 1, Batch 2000: Loss = 0.006143363658338785\n",
            "Epoch 1, Batch 2500: Loss = 0.02522009238600731\n",
            "Epoch 1, Batch 3000: Loss = 0.1259872019290924\n",
            "Epoch 1, Train_Loss = 0.1259872019290924, Test_Loss = 0.07093603909015656, Train_Acc = 0.9777166843414307, Test_Acc = 0.9825000166893005\n",
            "Epoch 2, Batch 500: Loss = 0.0006617597537115216\n",
            "Epoch 2, Batch 1000: Loss = 0.023345859721302986\n",
            "Epoch 2, Batch 1500: Loss = 0.006193316075950861\n",
            "Epoch 2, Batch 2000: Loss = 0.00514786085113883\n",
            "Epoch 2, Batch 2500: Loss = 0.050972260534763336\n",
            "Epoch 2, Batch 3000: Loss = 0.030789945274591446\n",
            "Epoch 2, Train_Loss = 0.030789945274591446, Test_Loss = 0.038089435547590256, Train_Acc = 0.9840999841690063, Test_Acc = 0.9799000024795532\n",
            "Epoch 3, Batch 500: Loss = 0.011481584049761295\n",
            "Epoch 3, Batch 1000: Loss = 0.0006975349970161915\n",
            "Epoch 3, Batch 1500: Loss = 0.007171654608100653\n",
            "Epoch 3, Batch 2000: Loss = 0.009105273522436619\n",
            "Epoch 3, Batch 2500: Loss = 0.0014811179134994745\n",
            "Epoch 3, Batch 3000: Loss = 0.04244281351566315\n",
            "Epoch 3, Train_Loss = 0.04244281351566315, Test_Loss = 0.008888911455869675, Train_Acc = 0.9869666695594788, Test_Acc = 0.9868000149726868\n",
            "Epoch 4, Batch 500: Loss = 0.03543202951550484\n",
            "Epoch 4, Batch 1000: Loss = 0.00549708865582943\n",
            "Epoch 4, Batch 1500: Loss = 0.0011014491319656372\n",
            "Epoch 4, Batch 2000: Loss = 0.0232939925044775\n",
            "Epoch 4, Batch 2500: Loss = 0.005864973179996014\n",
            "Epoch 4, Batch 3000: Loss = 0.0005352660664357245\n",
            "Epoch 4, Train_Loss = 0.0005352660664357245, Test_Loss = 0.0004939146456308663, Train_Acc = 0.9899166822433472, Test_Acc = 0.9864000082015991\n",
            "Epoch 5, Batch 500: Loss = 0.0008203432662412524\n",
            "Epoch 5, Batch 1000: Loss = 0.0005951427738182247\n",
            "Epoch 5, Batch 1500: Loss = 0.004642753396183252\n",
            "Epoch 5, Batch 2000: Loss = 0.07724326103925705\n",
            "Epoch 5, Batch 2500: Loss = 0.18756598234176636\n",
            "Epoch 5, Batch 3000: Loss = 0.0009930601809173822\n",
            "Epoch 5, Train_Loss = 0.0009930601809173822, Test_Loss = 0.007455050013959408, Train_Acc = 0.9915833473205566, Test_Acc = 0.9890999794006348\n",
            "Epoch 6, Batch 500: Loss = 3.103961716988124e-05\n",
            "Epoch 6, Batch 1000: Loss = 0.00016918433539103717\n",
            "Epoch 6, Batch 1500: Loss = 6.162474164739251e-05\n",
            "Epoch 6, Batch 2000: Loss = 0.1265374720096588\n",
            "Epoch 6, Batch 2500: Loss = 0.018418949097394943\n",
            "Epoch 6, Batch 3000: Loss = 0.3817793130874634\n",
            "Epoch 6, Train_Loss = 0.3817793130874634, Test_Loss = 0.0013367676874622703, Train_Acc = 0.9924833178520203, Test_Acc = 0.9882000088691711\n",
            "Epoch 7, Batch 500: Loss = 0.04230933636426926\n",
            "Epoch 7, Batch 1000: Loss = 0.014087107963860035\n",
            "Epoch 7, Batch 1500: Loss = 0.0003630743594840169\n",
            "Epoch 7, Batch 2000: Loss = 0.003042684867978096\n",
            "Epoch 7, Batch 2500: Loss = 0.004281769040971994\n",
            "Epoch 7, Batch 3000: Loss = 0.003249556990340352\n",
            "Epoch 7, Train_Loss = 0.003249556990340352, Test_Loss = 0.002516429638490081, Train_Acc = 0.9934333562850952, Test_Acc = 0.9900000095367432\n",
            "Epoch 8, Batch 500: Loss = 0.004000688903033733\n",
            "Epoch 8, Batch 1000: Loss = 0.003585169091820717\n",
            "Epoch 8, Batch 1500: Loss = 0.3573939800262451\n",
            "Epoch 8, Batch 2000: Loss = 0.00023791736748535186\n",
            "Epoch 8, Batch 2500: Loss = 0.0001163551933132112\n",
            "Epoch 8, Batch 3000: Loss = 0.1588422656059265\n",
            "Epoch 8, Train_Loss = 0.1588422656059265, Test_Loss = 0.0007202944252640009, Train_Acc = 0.9950833320617676, Test_Acc = 0.9883999824523926\n",
            "Epoch 9, Batch 500: Loss = 0.0032152701169252396\n",
            "Epoch 9, Batch 1000: Loss = 0.048386603593826294\n",
            "Epoch 9, Batch 1500: Loss = 4.289203934604302e-05\n",
            "Epoch 9, Batch 2000: Loss = 0.16758577525615692\n",
            "Epoch 9, Batch 2500: Loss = 0.00024176482111215591\n",
            "Epoch 9, Batch 3000: Loss = 0.001080735120922327\n",
            "Epoch 9, Train_Loss = 0.001080735120922327, Test_Loss = 2.6193078156211413e-05, Train_Acc = 0.9947166442871094, Test_Acc = 0.9901999831199646\n",
            "Training took: 5.502863295873007 minutes!\n"
          ]
        }
      ]
    }
  ]
}